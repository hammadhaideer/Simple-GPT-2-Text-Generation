Simple GPT-2 Text Generation

This project demonstrates a simple implementation of text generation using the GPT-2 model from the Hugging Face Transformers library.

Overview

Natural Language Generation (NLG) is a subfield of artificial intelligence and natural language processing that focuses on generating human-like text based on given input prompts. In this project, we utilize the GPT-2 model, a state-of-the-art transformer-based language model, to generate text based on user-provided prompts.

Features

Generates human-like text based on input prompts.
Utilizes pre-trained GPT-2 model from Hugging Face Transformers.
Customizable parameters such as maximum length and temperature for text generation.

Parameters

prompt: The input text prompt to generate text from.
max_length: Maximum length of the generated text (default: 100).
temperature: Controls the randomness of the generated text (default: 0.7).

Dependencies

transformers: Hugging Face library for natural language processing models.

License

This project is licensed under the MIT License - see the LICENSE file for details.
